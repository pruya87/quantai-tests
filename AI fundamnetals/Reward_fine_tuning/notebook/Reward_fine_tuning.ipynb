{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO2jVDoqCxpI"
   },
   "source": [
    "\n",
    "PFT = Base Model + Adapter Weights + Standard Supervised Loss\n",
    "RLHF = Base Model (frozen or fine-tuned) + Preference-based Loss + Reward Modeling (optional)\n",
    "\n",
    "| Method                | Base Model Frozen?           | Adapter Use | Notes                                                                    |\n",
    "| --------------------- | ---------------------------- | ----------- | ------------------------------------------------------------------------ |\n",
    "| **PEFT (LoRA/QLoRA)** | ✅ Yes                        | ✅ Yes       | Only adapter weights are trained. Efficient, low-rank updates.           |\n",
    "| **RLHF (PPO)**        | ❌ No (usually)               | ❌ Optional  | Full model is updated with policy gradients. LoRA *can* be used, though. |\n",
    "| **DPO / IPO**         | ✅ Often                      | ✅ Often     | Can work with frozen or adapter-based models. Efficient.                 |\n",
    "| **SPIN**              | ⚠️ One frozen, one trainable | ⚠️ Optional | Two models: reference (frozen) vs tuned. Learns margin between them.     |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PFT\n",
    "\n",
    "| Method         | What it changes                                                     | How it changes it                                            | Direct weight update?                      |\n",
    "| -------------- | ------------------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------ |\n",
    "| **PFT / LoRA** | Model's response to input (knowledge, task-specific patterns)       | By supervised fine-tuning on labeled data                    | ✅ Yes (via gradients, backprop)            |\n",
    "| **RLHF / DPO** | Model’s *preferences*, decision policy (how to rank/choose outputs) | By optimizing behavior to match human preferences or rewards | ✅ Yes (via reward gradients / preferences) |\n",
    "\n",
    "\n",
    "PFT techniques:\n",
    "\n",
    "LoRA:\n",
    "=====\n",
    "\n",
    "# Weights belong to a real number matrix of nxm\n",
    "# W ∈ R n×m\n",
    "# LoRAS creates 2 matrices of nxr and rxm dimensions\n",
    "# A ∈ ℝⁿˣʳ\n",
    "# B ∈ ℝʳˣᵐ\n",
    "# W' (FT Model) = W(frozen weights) + α ⋅ AB\n",
    "\n",
    "QLoRA:\n",
    "======\n",
    "Using a codebook to reduce the floating points of the model to reduce memory space\n",
    "\n",
    "## Unsloth techniques\n",
    "\n",
    "\n",
    "| Component         | Role                                          | Effect                                                          |\n",
    "| ----------------- | --------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **Full Model**    | Pretrained base model (large, high-capacity)  | Strong foundation, high accuracy                                |\n",
    "| **QLoRA**         | Quantized weights + low-rank adapters         | Memory & compute savings; efficient fine-tuning                 |\n",
    "| **Distillation**  | Train a smaller model to mimic a bigger model | Smaller, faster model that retains much of the original quality |\n",
    "| **Pruning**       | Remove unimportant weights                    | Smaller model, less computation                                 |\n",
    "| **Other Methods** | E.g., weight sharing, knowledge transfer      | Further compression and efficiency                              |\n",
    "\n",
    "RLHF/DPO/IPO/SPIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJb1X-_w92CD"
   },
   "outputs": [],
   "source": [
    "####### Step 1: Setup a toy multi-armed bandit environment\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, probs):\n",
    "        \"\"\"\n",
    "        probs: list of probabilities of reward=1 for each action\n",
    "        \"\"\"\n",
    "        self.probs = probs\n",
    "        self.n_actions = len(probs)\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 1 if np.random.rand() < self.probs[action] else 0\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIq--m1U99DB"
   },
   "outputs": [],
   "source": [
    "######## Step 2: Define a simple policy\n",
    "\n",
    "class SoftmaxPolicy:\n",
    "    def __init__(self, n_actions, lr=0.1):\n",
    "        self.logits = np.zeros(n_actions)  # initialized to zero (uniform)\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_probs(self):\n",
    "        exp_logits = np.exp(self.logits - np.max(self.logits))\n",
    "        return exp_logits / exp_logits.sum()\n",
    "\n",
    "    def select_action(self):\n",
    "        probs = self.get_probs()\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action, probs[action]\n",
    "\n",
    "    def update(self, action, reward, baseline=0):\n",
    "        probs = self.get_probs()\n",
    "        grad = -probs\n",
    "        grad[action] += 1  # gradient for selected action\n",
    "        # policy gradient update scaled by (reward - baseline)\n",
    "        self.logits += self.lr * grad * (reward - baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67kIG-eP-Ibb"
   },
   "outputs": [],
   "source": [
    "########## Step 3: Training loop with reward adjustment options\n",
    "\n",
    "def train_agent(env, policy, n_steps=500, reward_adjust='none', scale=1.0, clip_val=None, temp=1.0):\n",
    "    rewards_history = []\n",
    "    adjusted_rewards = []\n",
    "    cumulative_rewards = []\n",
    "    cum_reward = 0\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        action, prob = policy.select_action()\n",
    "        reward = env.step(action)\n",
    "\n",
    "        # Reward adjustments\n",
    "        adj_reward = reward\n",
    "\n",
    "        # 1. Scaling\n",
    "        if reward_adjust == 'scale':\n",
    "            adj_reward = reward * scale\n",
    "\n",
    "        # 2. Clipping\n",
    "        elif reward_adjust == 'clip':\n",
    "            adj_reward = np.clip(reward, 0, clip_val)\n",
    "\n",
    "        # 3. Temperature scaling (softmax temp on rewards, hypothetical)\n",
    "        elif reward_adjust == 'temp':\n",
    "            # for a bandit reward of 0 or 1, let's make it continuous by applying a temperature-like effect\n",
    "            # This is illustrative: scaled_reward = exp(reward/temp) normalized in [0,1]\n",
    "            adj_reward = np.exp(reward / temp)\n",
    "            adj_reward = adj_reward / np.exp(1 / temp)\n",
    "\n",
    "        policy.update(action, adj_reward)\n",
    "\n",
    "        cum_reward += reward\n",
    "        rewards_history.append(reward)\n",
    "        adjusted_rewards.append(adj_reward)\n",
    "        cumulative_rewards.append(cum_reward)\n",
    "\n",
    "    return rewards_history, adjusted_rewards, cumulative_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHyWQ6ip-LS1"
   },
   "outputs": [],
   "source": [
    "########  Step 4: Run and compare\n",
    "\n",
    "env = MultiArmedBandit([0.2, 0.5, 0.8])\n",
    "policy = SoftmaxPolicy(env.n_actions, lr=0.1)\n",
    "\n",
    "n_steps = 500\n",
    "results = {}\n",
    "\n",
    "for adj in ['none', 'scale', 'clip', 'temp']:\n",
    "    policy = SoftmaxPolicy(env.n_actions, lr=0.1)\n",
    "    if adj == 'scale':\n",
    "        r, ar, cr = train_agent(env, policy, n_steps, reward_adjust=adj, scale=2.0)\n",
    "    elif adj == 'clip':\n",
    "        r, ar, cr = train_agent(env, policy, n_steps, reward_adjust=adj, clip_val=0.5)\n",
    "    elif adj == 'temp':\n",
    "        r, ar, cr = train_agent(env, policy, n_steps, reward_adjust=adj, temp=0.5)\n",
    "    else:\n",
    "        r, ar, cr = train_agent(env, policy, n_steps, reward_adjust=adj)\n",
    "    results[adj] = (r, ar, cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "PerAN0MO-Qwl",
    "outputId": "5306c15d-5d07-4160-e53e-4cdc89d30540"
   },
   "outputs": [],
   "source": [
    "########  Step 5: Plot\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "for adj in results:\n",
    "    plt.plot(results[adj][0], label=adj)\n",
    "plt.title(\"Original Rewards\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "for adj in results:\n",
    "    plt.plot(results[adj][1], label=adj)\n",
    "plt.title(\"Adjusted Rewards\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Adjusted Reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "for adj in results:\n",
    "    plt.plot(results[adj][2], label=adj)\n",
    "plt.title(\"Cumulative Rewards\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Sum of rewards\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWwcNvbGMtpM"
   },
   "source": [
    "| Method | Reward Signal Type    | Training Style       | Label Requirement      |\n",
    "| ------ | --------------------- | -------------------- | ---------------------- |\n",
    "| PPO    | Explicit rewards      | Policy gradient (RL) | Needs reward function  |\n",
    "| DPO    | Pairwise preferences  | Direct optimization  | Needs preference pairs |\n",
    "| IPO    | Implicit feedback     | Reward model + RL    | Indirect signals       |\n",
    "| SPIN   | Self-supervised prefs | Joint learning       | No labels needed       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTtIouagFIp7"
   },
   "source": [
    "PPO  \n",
    "  \n",
    "Policy π_old  --> Sample actions in env --> Collect rewards R  \n",
    "         ↓  \n",
    "Compute advantage A = R - baseline  \n",
    "         ↓  \n",
    "Compute clipped (avoid big changes) surrogate loss L_clip(π, π_old, A)  \n",
    "         ↓  \n",
    "Gradient ascent on L_clip to update policy π_new  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjr-183ZFTTU"
   },
   "source": [
    "DPO  \n",
    "  \n",
    "Policy π  --> Generate outputs  \n",
    "         ↓  \n",
    "Get pairwise preferences from annotators or reward model (Pair Accepted/Rejected) and computes the reward from delta\n",
    "         ↓  \n",
    "Compute preference loss L_pref based on likelihood of preferred outputs  \n",
    "         ↓  \n",
    "Update π by maximizing L_pref (no policy gradient or RL needed)  \n",
    "\n",
    "Math\n",
    "\n",
    "π0​ = original pretrained model (fixed)\n",
    "πθ​ = current model being trained/updated\n",
    "\n",
    "The loss for one pair is:\n",
    "L(θ)=−log⁡σ(α(log⁡πθ(x+)−log⁡πθ(x−)))\n",
    "\n",
    "Where:\n",
    "\n",
    "    σσ is the sigmoid function: σ(z)=11+e−zσ(z)=1+e−z1​\n",
    "\n",
    "    α>0α>0 is a scaling factor controlling how “strongly” we push the preference margin\n",
    "\n",
    "    log⁡πθ(⋅)logπθ​(⋅) is the log-likelihood of the model output  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LexXWKRxFvf_"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
