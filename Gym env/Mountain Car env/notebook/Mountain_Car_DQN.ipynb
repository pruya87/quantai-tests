{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7lFBby6iDdr"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"MountainCar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e28ngtxpJvbj",
    "outputId": "c295f105-9714-4819-a3a9-4efdbbe71f28"
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra] imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl57nnip5nmO"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from IPython.display import Image, display,clear_output\n",
    "\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuQFJYyGbv-j",
    "outputId": "ada58e18-dc80-447d-fdb0-38e4fd6fd0f9"
   },
   "outputs": [],
   "source": [
    "# Lookup which scenarios in gymnasium are well suited for DQN (linear and lowdimensional (2D))\n",
    "\n",
    "for env_spec in gym.registry.values():\n",
    "    if hasattr(env_spec, \"entry_point\"):\n",
    "        try:\n",
    "            env = gym.make(env_spec.id)\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                print(env_spec.id)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6kfKyISBkir"
   },
   "outputs": [],
   "source": [
    "# The structure looks something like this:\n",
    "\"\"\"\n",
    "DummyVecEnv\n",
    "└── Monitor\n",
    "    └── TimeLimit\n",
    "        └── CartPoleEnv\n",
    "\"\"\"\n",
    "\n",
    "#We need to umwrap the structure until we reach the instance of monitor\n",
    "def unwrap_monitor(env):\n",
    "    \"\"\"Recursively unwrap env until we find a Monitor.\"\"\"\n",
    "    current = env\n",
    "    while not isinstance(current, Monitor):\n",
    "        if hasattr(current, 'env'):\n",
    "            current = current.env\n",
    "        else:\n",
    "            raise ValueError(\"Monitor wrapper not found in the environment stack.\")\n",
    "    return current\n",
    "\n",
    "# ✅ A Gym environment → wrapped in a Monitor(logs metrics, rewards... for episodes) → then wrapped in a VecEnv (like DummyVecEnv)\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "        env = RewardShapingWrapper(env)\n",
    "        env = Monitor(env)  # Wrap with Monitor first\n",
    "        return env\n",
    "    return DummyVecEnv([_init])\n",
    "\n",
    "class RewardShapingWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        position = obs[0]\n",
    "        shaped_reward = reward + abs(position + 0.5)  # Encourage movement far from default positions\n",
    "        return obs, shaped_reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class LiveRenderCallback(BaseCallback):\n",
    "    def __init__(self, render_freq=1000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            shot = self.training_env.render(mode=\"rgb_array\")\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(shot)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8wrsoRahIVfA",
    "outputId": "56f40bc6-8498-4f78-e33c-c9897a38cf5d"
   },
   "outputs": [],
   "source": [
    "# Create and wrap environment (Monitor helps log rewards)\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "#env = DummyVecEnv([lambda: Monitor(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"))])\n",
    "\n",
    "\n",
    "# Create eval environment (for callback evaluation)\n",
    "eval_env = make_env()\n",
    "\n",
    "# Create directory to save models and videos\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "# Create the DQN agent\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    tensorboard_log=\"./dqn_cartpole_tensorboard/\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Setup evaluation callback to save best model during training\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./models/best_model\",\n",
    "    log_path=\"./models/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "# Train the model (you can increase total_timesteps for better results)\n",
    "model.learn(total_timesteps=100000, callback=[eval_callback, LiveRenderCallback()])\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"models/dqn_cartpole_final\")\n",
    "\n",
    "# Load best saved model\n",
    "model = DQN.load(\"./models/best_model/best_model.zip\", env=env)\n",
    "\n",
    "# Run one episode and record frames for GIF\n",
    "obs = env.reset()\n",
    "done = False\n",
    "frames = []\n",
    "done = False\n",
    "while not done:\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "# Save GIF\n",
    "gif_path = \"videos/cartpole_agent.gif\"\n",
    "imageio.mimsave(gif_path, frames, fps=30)\n",
    "\n",
    "# Display GIF inline (Jupyter/Colab)\n",
    "display(Image(filename=gif_path))\n",
    "\n",
    "# Plot episode rewards logged during training\n",
    "monitor_env = unwrap_monitor(env.envs[0]) #Unwraps the structure of the first environment [0] until it finds the monitor\n",
    "results = monitor_env.get_episode_rewards()\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training Rewards per Episode\")\n",
    "plt.show()\n",
    "\n",
    "# Instructions to run tensorboard (if desired):\n",
    "# In terminal: tensorboard --logdir ./dqn_cartpole_tensorboard/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukLO09TPNM89",
    "outputId": "691029ef-3306-4de6-d3a3-8060e3278609"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+-------------+ (step)   +-----------------+          +--------------------+\n",
    "|             | action   |                 | new obs  |                    |\n",
    "|   Agent     +--------->+   Environment   +--------->+     Agent again    |\n",
    "|  (predict)  |          |                 | reward   |                    |\n",
    "+-------------+          +-----------------+  done?   +--------------------+\n",
    "                                  ^                               |\n",
    "                                  |<-------------[loop]-----------+\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## Hyperparameter tuning with manual search\n",
    "\n",
    "import itertools\n",
    "\n",
    "def train_dqn_with_params(env, params):\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        gamma=params[\"gamma\"],\n",
    "        verbose=0,\n",
    "    )\n",
    "    model.learn(total_timesteps=10000)\n",
    "    return model\n",
    "\n",
    "search_space = {\n",
    "    \"learning_rate\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"gamma\": [0.95, 0.99]\n",
    "}\n",
    "\n",
    "def evaluate_model(model, env, n_episodes=5):\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return sum(rewards) / n_episodes\n",
    "\n",
    "best_params = None\n",
    "best_reward = -float('inf')\n",
    "results = []\n",
    "\n",
    "for values in itertools.product(*search_space.values()):\n",
    "    params = dict(zip(search_space.keys(), values))\n",
    "    print(f\"Training with params: {params}\")\n",
    "    model = train_dqn_with_params(env, params)\n",
    "    avg_reward = evaluate_model(model, env)\n",
    "    print(f\"Average reward: {avg_reward}\")\n",
    "\n",
    "    results.append((params, avg_reward))\n",
    "    if avg_reward > best_reward:\n",
    "        best_reward = avg_reward\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best params: {best_params} with average reward {best_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnP1cWqfPcN9",
    "outputId": "cf7f87c3-88ae-48d8-99e8-a8eb1d300e3e"
   },
   "outputs": [],
   "source": [
    "## Hyperparameter tuning with random search\n",
    "\n",
    "import random\n",
    "\n",
    "def random_search(env, search_space, n_trials=10):\n",
    "    results = []\n",
    "    for _ in range(n_trials):\n",
    "        params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "        print(f\"Training with params: {params}\")\n",
    "        model = train_dqn_with_params(env, params)\n",
    "        # Evaluate model here\n",
    "        results.append((params, model))\n",
    "    return results\n",
    "\n",
    "# Use same search_space as above\n",
    "results = random_search(env, search_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilEA1LGiP4S0",
    "outputId": "e3651fc0-8e7d-46ca-9b9a-28bcc138f1cc"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0rrIeb-PmDp",
    "outputId": "fd3871d3-7a11-4af9-cd33-0caaf8ebb751"
   },
   "outputs": [],
   "source": [
    "## Hyperparameter tuning automated with optuna\n",
    "\n",
    "import optuna\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"gamma\": trial.suggest_uniform(\"gamma\", 0.9, 0.999),\n",
    "    }\n",
    "    model = DQN(\"MlpPolicy\", env, **params, verbose=0)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
