{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7lFBby6iDdr"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"Acrobot-v1\"\n",
    "N_ITERATIONS = 3\n",
    "TRAIN_STEPS = 100000\n",
    "ENV_VAR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "R9j57MFQgPwZ",
    "outputId": "08e8b794-3838-4637-858c-4d54cb56fca3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Values in an observation\n",
    "\n",
    "cos(theta1) — cosine of the first joint angle\n",
    "\n",
    "sin(theta1) — sine of the first joint angle\n",
    "\n",
    "cos(theta2) — cosine of the second joint angle\n",
    "\n",
    "sin(theta2) — sine of the second joint angle\n",
    "\n",
    "theta1_dot — angular velocity of the first joint\n",
    "\n",
    "theta2_dot — angular velocity of the second joint\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e28ngtxpJvbj",
    "outputId": "5d0f5a6f-5b99-4c73-becd-79038d5e0af7"
   },
   "outputs": [],
   "source": [
    "#### Implemented on python. No wrapper needed\n",
    "\n",
    "#!apt-get install -y swig\n",
    "\n",
    "#!pip install imageio box2d-py gymnasium[box2d]\n",
    "\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl57nnip5nmO"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from IPython.display import Image, display,clear_output\n",
    "from PIL import Image as PILImage, ImageDraw, ImageFont\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from collections import Counter\n",
    "\n",
    "import os, builtins, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuQFJYyGbv-j",
    "outputId": "5073b548-0dcb-4a43-bcd6-da15ce5249bc"
   },
   "outputs": [],
   "source": [
    "# Lookup which scenarios in gymnasium are well suited for DQN (linear and lowdimensional (2D))\n",
    "\n",
    "for env_spec in gym.registry.values():\n",
    "    if hasattr(env_spec, \"entry_point\"):\n",
    "        try:\n",
    "            env = gym.make(env_spec.id)\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                print(env_spec.id)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6kfKyISBkir"
   },
   "outputs": [],
   "source": [
    "# The structure looks something like this:\n",
    "\"\"\"\n",
    "DummyVecEnv\n",
    "└── Monitor\n",
    "    └── TimeLimit\n",
    "        └── CartPoleEnv\n",
    "\"\"\"\n",
    "\n",
    "#Custom reward system wrapper\n",
    "\n",
    "class RewardShapingWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # No shaping applied for now — default reward passed through\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "#Custom Live Render Function\n",
    "\n",
    "class LiveRenderCallback(BaseCallback):\n",
    "    def __init__(self, render_freq=1000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            shot = self.training_env.render(mode=\"rgb_array\")\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(shot)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        return True\n",
    "\n",
    "\n",
    "#We need to umwrap the structure until we reach the instance of monitor\n",
    "def unwrap_monitor(env):\n",
    "    \"\"\"Recursively unwrap env until we find a Monitor.\"\"\"\n",
    "    current = env\n",
    "    while not isinstance(current, Monitor):\n",
    "        if hasattr(current, 'env'):\n",
    "            current = current.env\n",
    "        else:\n",
    "            raise ValueError(\"Monitor wrapper not found in the environment stack.\")\n",
    "    return current\n",
    "\n",
    "# ✅ A Gym environment → wrapped in a Monitor(logs metrics, rewards... for episodes) → then wrapped in a VecEnv (like DummyVecEnv)\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "        env = RewardShapingWrapper(env)\n",
    "        env = Monitor(env)  # Wrap with Monitor first\n",
    "        return env\n",
    "    return DummyVecEnv([_init])\n",
    "\n",
    "def test_agent(env=None, model=None, iterations=1, env_name=\"env\", annotate_fn=None, font=None):\n",
    "    import builtins\n",
    "    if env is None:\n",
    "        env = globals().get(\"env\")\n",
    "    if model is None:\n",
    "        model = globals().get(\"model\")\n",
    "\n",
    "    if env is None or model is None:\n",
    "        raise ValueError(\"env and model must be defined or passed explicitly\")\n",
    "\n",
    "    if font is None:\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        obs = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        action_counter = Counter()\n",
    "        frames = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            frame = env.render()\n",
    "\n",
    "            img = PILImage.fromarray(frame)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, rewards, dones, infos = env.step(action)\n",
    "            done = dones[0]  # Since it's a single environment inside DummyVecEnv\n",
    "            reward = rewards[0]\n",
    "            info = infos[0]\n",
    "\n",
    "            total_reward += reward\n",
    "            action_counter[action.item()] += 1\n",
    "            step += 1\n",
    "\n",
    "            # Unwrap observation if needed\n",
    "            if isinstance(obs, np.ndarray) and obs.ndim == 2:\n",
    "                obs_values = obs[0]\n",
    "            else:\n",
    "                obs_values = obs\n",
    "\n",
    "            # ✍️ Annotation via custom function or fallback\n",
    "            if annotate_fn:\n",
    "                text_lines = annotate_fn(obs_values, step, total_reward, info)\n",
    "            else:\n",
    "                text_lines = [\n",
    "                    f\"Step: {step}\",\n",
    "                    f\"Total Reward: {total_reward:.2f}\"\n",
    "                ]\n",
    "\n",
    "\n",
    "            y_offset = 10\n",
    "            for line in text_lines:\n",
    "                draw.text((10, y_offset), line, fill=(0, 0, 0), font=font)\n",
    "                y_offset += 15\n",
    "\n",
    "            frames.append(np.array(img))\n",
    "\n",
    "        # Save and display GIF\n",
    "        gif_path = f\"videos/{env_name.lower()}_episode_{i}_agent.gif\"\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        display(Image(filename=gif_path))\n",
    "\n",
    "        # Unwrap and plot rewards\n",
    "        monitor_env = unwrap_monitor(env.envs[0])\n",
    "        results = monitor_env.get_episode_rewards()\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        print(f\"\\n▶️ Episode {i}\")\n",
    "        print(f\"Total reward: {total_reward:.2f}\")\n",
    "        print(f\"Episode length: {step} steps\")\n",
    "        print(\"Action distribution:\", dict(action_counter))\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(results)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Training Rewards per Episode\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.bar(action_counter.keys(), action_counter.values())\n",
    "        plt.xlabel(\"Action\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Action Distribution\")\n",
    "        plt.show()\n",
    "\n",
    "def acrobot_annotate_fn(obs, step, total_reward, info):\n",
    "    # Recover angles from cos and sin\n",
    "    theta1 = math.atan2(obs[1], obs[0])\n",
    "    theta2 = math.atan2(obs[3], obs[2])\n",
    "    theta1_dot = obs[4]\n",
    "    theta2_dot = obs[5]\n",
    "\n",
    "    # Format angles in degrees for easier reading\n",
    "    theta1_deg = math.degrees(theta1)\n",
    "    theta2_deg = math.degrees(theta2)\n",
    "\n",
    "    text_lines = [\n",
    "        f\"Step: {step}\",\n",
    "        f\"Total Reward: {total_reward:.2f}\",\n",
    "        f\"Theta1: {theta1_deg:.1f}°\",\n",
    "        f\"Theta2: {theta2_deg:.1f}°\",\n",
    "        f\"Theta1 dot: {theta1_dot:.2f}\",\n",
    "        f\"Theta2 dot: {theta2_dot:.2f}\",\n",
    "    ]\n",
    "    return text_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8wrsoRahIVfA",
    "outputId": "a1d548b6-704e-44d6-fe5f-694b96ac2094"
   },
   "outputs": [],
   "source": [
    "# Create and wrap environment (Monitor helps log rewards)\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "# Create eval environment (for callback evaluation)\n",
    "eval_env = make_env()\n",
    "\n",
    "# Create directory to save models and videos\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "# Create the DQN agent\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    tensorboard_log=\"./dqn_cartpole_tensorboard/\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Setup evaluation callback to save best model during training\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./models/best_model\",\n",
    "    log_path=\"./models/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "# Train the model (you can increase total_timesteps for better results)\n",
    "model.learn(total_timesteps=TRAIN_STEPS, callback=[eval_callback, LiveRenderCallback()])\n",
    "\n",
    "# Save the final model\n",
    "model.save(f\"models/dqn_{ENV_NAME}_final\")\n",
    "\n",
    "# Load best saved model\n",
    "model = DQN.load(\"./models/best_model/best_model.zip\", env=env)\n",
    "\n",
    "test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5-OeLar4SHyD",
    "outputId": "99abadc5-ff15-4bc6-913e-8b214d5f2837"
   },
   "outputs": [],
   "source": [
    "test_agent(env=env, model=model, iterations=N_ITERATIONS, env_name=\"Acrobot\", annotate_fn=acrobot_annotate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukLO09TPNM89"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+-------------+ (step)   +-----------------+          +--------------------+\n",
    "|             | action   |                 | new obs  |                    |\n",
    "|   Agent     +--------->+   Environment   +--------->+     Agent again    |\n",
    "|  (predict)  |          |                 | reward   |                    |\n",
    "+-------------+          +-----------------+  done?   +--------------------+\n",
    "                                  ^                               |\n",
    "                                  |<-------------[loop]-----------+\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## Hyperparameter tuning with manual search\n",
    "\n",
    "import itertools\n",
    "\n",
    "def train_dqn_with_params(env, params):\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        gamma=params[\"gamma\"],\n",
    "        verbose=0,\n",
    "    )\n",
    "    model.learn(total_timesteps=10000)\n",
    "    return model\n",
    "\n",
    "search_space = {\n",
    "    \"learning_rate\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"gamma\": [0.95, 0.99]\n",
    "}\n",
    "\n",
    "def evaluate_model(model, env, n_episodes=5):\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return sum(rewards) / n_episodes\n",
    "\n",
    "best_params = None\n",
    "best_reward = -float('inf')\n",
    "results = []\n",
    "\n",
    "for values in itertools.product(*search_space.values()):\n",
    "    params = dict(zip(search_space.keys(), values))\n",
    "    print(f\"Training with params: {params}\")\n",
    "    model = train_dqn_with_params(env, params)\n",
    "    avg_reward =  (model, env)\n",
    "    print(f\"Average reward: {avg_reward}\")\n",
    "\n",
    "    results.append((params, avg_reward))\n",
    "    if avg_reward > best_reward:\n",
    "        best_reward = avg_reward\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best params: {best_params} with average reward {best_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnP1cWqfPcN9"
   },
   "outputs": [],
   "source": [
    "## Hyperparameter tuning with random search\n",
    "\n",
    "import random\n",
    "\n",
    "def random_search(env, search_space, n_trials=10):\n",
    "    results = []\n",
    "    for _ in range(n_trials):\n",
    "        params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "        print(f\"Training with params: {params}\")\n",
    "        model = train_dqn_with_params(env, params)\n",
    "        # Evaluate model here\n",
    "        results.append((params, model))\n",
    "    return results\n",
    "\n",
    "# Use same search_space as above\n",
    "results = random_search(env, search_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilEA1LGiP4S0"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0rrIeb-PmDp"
   },
   "outputs": [],
   "source": [
    "## Hyperparameter tuning automated with optuna\n",
    "\n",
    "import optuna\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"gamma\": trial.suggest_uniform(\"gamma\", 0.9, 0.999),\n",
    "    }\n",
    "    model = DQN(\"MlpPolicy\", env, **params, verbose=0)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
