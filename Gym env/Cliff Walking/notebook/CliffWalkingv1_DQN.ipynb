{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7lFBby6iDdr"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"CliffWalking-v0\"\n",
    "N_ITERATIONS = 3\n",
    "N_BATCHES = 3\n",
    "TRAIN_STEPS = 100000\n",
    "ENV_VAR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "R9j57MFQgPwZ",
    "outputId": "84bdbf60-a6b7-450c-c85d-40809629f4cd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Values in an observation\n",
    "\n",
    "cos(theta1) — cosine of the first joint angle\n",
    "\n",
    "sin(theta1) — sine of the first joint angle\n",
    "\n",
    "cos(theta2) — cosine of the second joint angle\n",
    "\n",
    "sin(theta2) — sine of the second joint angle\n",
    "\n",
    "theta1_dot — angular velocity of the first joint\n",
    "\n",
    "theta2_dot — angular velocity of the second joint\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e28ngtxpJvbj",
    "outputId": "bf255a6d-b890-45e3-beb4-80ac80ab65e7"
   },
   "outputs": [],
   "source": [
    "#### Implemented on python. No wrapper needed\n",
    "\n",
    "#!apt-get install -y swig\n",
    "\n",
    "#!pip install imageio box2d-py gymnasium[box2d]\n",
    "\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl57nnip5nmO"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from IPython.display import Image, display,clear_output\n",
    "from PIL import Image as PILImage, ImageDraw, ImageFont\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from collections import Counter\n",
    "\n",
    "import os, builtins, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuQFJYyGbv-j",
    "outputId": "32a4b6b6-ad3d-4ba6-977e-c64641311e88"
   },
   "outputs": [],
   "source": [
    "# Lookup which scenarios in gymnasium are well suited for DQN (linear and lowdimensional (2D))\n",
    "\n",
    "for env_spec in gym.registry.values():\n",
    "    if hasattr(env_spec, \"entry_point\"):\n",
    "        try:\n",
    "            env = gym.make(env_spec.id)\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                print(env_spec.id)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6kfKyISBkir"
   },
   "outputs": [],
   "source": [
    "# The structure looks something like this:\n",
    "\"\"\"\n",
    "DummyVecEnv\n",
    "└── Monitor\n",
    "    └── TimeLimit\n",
    "        └── CartPoleEnv\n",
    "\"\"\"\n",
    "\n",
    "#Custom reward system wrapper\n",
    "\n",
    "class RewardShapingWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "\n",
    "        # Example: currently no modification, just pass through\n",
    "        # You can customize reward shaping here:\n",
    "        # if reward == -100:\n",
    "        #     reward = -10  # soften cliff penalty, for instance\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "#Custom Live Render Function\n",
    "\n",
    "class LiveRenderCallback(BaseCallback):\n",
    "    def __init__(self, render_freq=1000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            shot = self.training_env.render(mode=\"rgb_array\")\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(shot)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        return True\n",
    "\n",
    "\n",
    "#We need to umwrap the structure until we reach the instance of monitor\n",
    "def unwrap_monitor(env):\n",
    "    \"\"\"Recursively unwrap env until we find a Monitor.\"\"\"\n",
    "    current = env\n",
    "    while not isinstance(current, Monitor):\n",
    "        if hasattr(current, 'env'):\n",
    "            current = current.env\n",
    "        else:\n",
    "            raise ValueError(\"Monitor wrapper not found in the environment stack.\")\n",
    "    return current\n",
    "\n",
    "# ✅ A Gym environment → wrapped in a Monitor(logs metrics, rewards... for episodes) →\n",
    "# then wrapped in a Reward Shaper  →  then wrapped in a VecEnv (like DummyVecEnv)\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "        env = RewardShapingWrapper(env)\n",
    "        env = Monitor(env)  # Wrap with Monitor first\n",
    "        return env\n",
    "    return DummyVecEnv([_init])\n",
    "\n",
    "def test_agent(env=None, model=None, iterations=1, env_name=\"env\", annotate_fn=None, font=None):\n",
    "    import builtins\n",
    "    if env is None:\n",
    "        env = globals().get(\"env\")\n",
    "    if model is None:\n",
    "        model = globals().get(\"model\")\n",
    "\n",
    "    if env is None or model is None:\n",
    "        raise ValueError(\"env and model must be defined or passed explicitly\")\n",
    "\n",
    "    if font is None:\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        obs = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        action_counter = Counter()\n",
    "        frames = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            frame = env.render()\n",
    "\n",
    "            img = PILImage.fromarray(frame)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "\n",
    "            # Predicts next action based on the current observation\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            # Takes the step (action) and returns the new observation, rewads and state\n",
    "            obs, rewards, dones, infos = env.step(action)\n",
    "            done = dones[0]  # Since it's a single environment [0] inside DummyVecEnv\n",
    "            reward = rewards[0]\n",
    "            info = infos[0]\n",
    "\n",
    "            total_reward += reward\n",
    "            action_counter[action.item()] += 1\n",
    "            step += 1\n",
    "\n",
    "            # Unwrap observation if needed\n",
    "            if isinstance(obs, np.ndarray) and obs.ndim == 2:\n",
    "                obs_values = obs[0]\n",
    "            else:\n",
    "                obs_values = obs\n",
    "\n",
    "            # ✍️ Annotation via custom function or fallback\n",
    "            if annotate_fn:\n",
    "                text_lines = annotate_fn(obs_values, step, total_reward, info)\n",
    "            else:\n",
    "                text_lines = [\n",
    "                    f\"Step: {step}\",\n",
    "                    f\"Total Reward: {total_reward:.2f}\"\n",
    "                ]\n",
    "\n",
    "\n",
    "            y_offset = 10\n",
    "            for line in text_lines:\n",
    "                draw.text((10, y_offset), line, fill=(0, 0, 0), font=font)\n",
    "                y_offset += 15\n",
    "\n",
    "            frames.append(np.array(img))\n",
    "\n",
    "        # Save and display GIF\n",
    "        gif_path = f\"videos/{env_name.lower()}_episode_{i}_agent.gif\"\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        display(Image(filename=gif_path))\n",
    "\n",
    "        # Unwrap and plot rewards\n",
    "        monitor_env = unwrap_monitor(env.envs[0])\n",
    "        results = monitor_env.get_episode_rewards()\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        print(f\"\\n▶️ Episode {i}\")\n",
    "        print(f\"Total reward: {total_reward:.2f}\")\n",
    "        print(f\"Episode length: {step} steps\")\n",
    "        print(\"Action distribution:\", dict(action_counter))\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(results)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Training Rewards per Episode\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.bar(action_counter.keys(), action_counter.values())\n",
    "        plt.xlabel(\"Action\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Action Distribution\")\n",
    "        plt.show()\n",
    "\n",
    "def train_multiple_batches(model_class, env_func, total_timesteps=300_000, n_batches=3):\n",
    "    all_results = []\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        print(f\"\\n--- Starting batch {batch_idx + 1} ---\")\n",
    "        env = env_func()\n",
    "        model = model_class(\"MlpPolicy\", env, verbose=1)\n",
    "        model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "        monitor_env = unwrap_monitor(env.envs[0])\n",
    "        batch_rewards = monitor_env.get_episode_rewards()\n",
    "        all_results.append(batch_rewards)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def plot_batch_results(all_results):\n",
    "    max_len = max(len(r) for r in all_results)\n",
    "    padded = [np.pad(r, (0, max_len - len(r)), constant_values=np.nan) for r in all_results]\n",
    "    stacked = np.vstack(padded)\n",
    "\n",
    "    mean_rewards = np.nanmean(stacked, axis=0)\n",
    "    std_rewards = np.nanstd(stacked, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(mean_rewards, label='Mean reward')\n",
    "    plt.fill_between(range(max_len), mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Rewards Across Batches (Mean ± Std)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def frozenlake_annotate_fn(obs, step, total_reward, info):\n",
    "    text_lines = [\n",
    "        f\"Step: {step}\",\n",
    "        f\"Total Reward: {total_reward:.2f}\",\n",
    "        f\"Position: {obs}\",  # obs is a scalar tile index\n",
    "    ]\n",
    "    return text_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "8wrsoRahIVfA",
    "outputId": "2abbfa36-0bd6-4145-f849-22f948beafa7"
   },
   "outputs": [],
   "source": [
    "# Create and wrap environment (Monitor helps log rewards)\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "# Create eval environment (for callback evaluation)\n",
    "eval_env = make_env()\n",
    "\n",
    "# Create directory to save models and videos\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "# Create the DQN agent with a Multi Layer Perceptron\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-3,  # Slightly higher to help early learning\n",
    "    buffer_size=100_000,  # More transitions to remember rare successes\n",
    "    learning_starts=1_000,\n",
    "    batch_size=64,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=500,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.1,\n",
    "    exploration_fraction=0.2,  # Explore for first 20% of training\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./frozenlake_dqn_tensorboard/\",\n",
    "    policy_kwargs=dict(net_arch=[128, 128]),  # Slightly deeper net\n",
    ")\n",
    "\n",
    "# Setup evaluation callback to save best model during training\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./models/best_model\",\n",
    "    log_path=\"./models/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "# Train the model (you can increase total_timesteps for better results)\n",
    "model.learn(total_timesteps=TRAIN_STEPS, callback=[eval_callback, LiveRenderCallback()])\n",
    "\n",
    "# Save the final model\n",
    "model.save(f\"models/dqn_{ENV_NAME}_final\")\n",
    "\n",
    "# Load best saved model\n",
    "model = DQN.load(\"./models/best_model/best_model.zip\", env=env)\n",
    "\n",
    "test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-OeLar4SHyD"
   },
   "outputs": [],
   "source": [
    "test_agent(env=env, model=model, iterations=N_ITERATIONS, env_name=\"FrozenLake\", annotate_fn=frozenlake_annotate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2c_Zyvy5_FY"
   },
   "outputs": [],
   "source": [
    "batch_results = train_multiple_batches(DQN, make_env, total_timesteps=TRAIN_STEPS, n_batches=N_BATCHES)\n",
    "plot_batch_results(batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukLO09TPNM89"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+-------------+ (step)   +-----------------+          +--------------------+\n",
    "|             | action   |                 | new obs  |                    |\n",
    "|   Agent     +--------->+   Environment   +--------->+     Agent again    |\n",
    "|  (predict)  |          |                 | reward   |                    |\n",
    "+-------------+          +-----------------+  done?   +--------------------+\n",
    "                                  ^                               |\n",
    "                                  |<-------------[loop]-----------+\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## Hyperparameter tuning with manual search\n",
    "\n",
    "import itertools\n",
    "\n",
    "def train_dqn_with_params(env, params):\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        gamma=params[\"gamma\"],\n",
    "        verbose=0,\n",
    "    )\n",
    "    model.learn(total_timesteps=10000)\n",
    "    return model\n",
    "\n",
    "search_space = {\n",
    "    \"learning_rate\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"gamma\": [0.95, 0.99]\n",
    "}\n",
    "\n",
    "def evaluate_model(model, env, n_episodes=5):\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return sum(rewards) / n_episodes\n",
    "\n",
    "best_params = None\n",
    "best_reward = -float('inf')\n",
    "results = []\n",
    "\n",
    "for values in itertools.product(*search_space.values()):\n",
    "    params = dict(zip(search_space.keys(), values))\n",
    "    print(f\"Training with params: {params}\")\n",
    "    model = train_dqn_with_params(env, params)\n",
    "    avg_reward =  (model, env)\n",
    "    print(f\"Average reward: {avg_reward}\")\n",
    "\n",
    "    results.append((params, avg_reward))\n",
    "    if avg_reward > best_reward:\n",
    "        best_reward = avg_reward\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best params: {best_params} with average reward {best_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnP1cWqfPcN9"
   },
   "outputs": [],
   "source": [
    "## Hyperparameter tuning with random search\n",
    "\n",
    "import random\n",
    "\n",
    "def random_search(env, search_space, n_trials=10):\n",
    "    results = []\n",
    "    for _ in range(n_trials):\n",
    "        params = {k: random.choice(v) for k, v in search_space.items()}\n",
    "        print(f\"Training with params: {params}\")\n",
    "        model = train_dqn_with_params(env, params)\n",
    "        # Evaluate model here\n",
    "        results.append((params, model))\n",
    "    return results\n",
    "\n",
    "# Use same search_space as above\n",
    "results = random_search(env, search_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilEA1LGiP4S0"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0rrIeb-PmDp"
   },
   "outputs": [],
   "source": [
    "## Hyperparameter tuning automated with optuna\n",
    "\n",
    "import optuna\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"gamma\": trial.suggest_uniform(\"gamma\", 0.9, 0.999),\n",
    "    }\n",
    "    model = DQN(\"MlpPolicy\", env, **params, verbose=0)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
